---
title: "SBF vignette"
author: "Amal Thomas"
output:
  pdf_document:
    toc: yes
    number_section: yes
  knitr:::html_vignette:
    toc: yes
  html_document:
    toc: yes
    df_print: paged
bibliography: references.bib
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{SBF_vignette}
  %\VignetteEncoding{UTF-8}
---

<style>
body {
text-align: justify}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{tr}}

# Background

Joint matrix factorization facilitates the comparison of
expression profiles from different species without using gene mapping.
Transforming gene expression profiles into reduced eigengene space using
singular value decomposition (SVD) has been shown to capture meaningful
biological information [@alter2000singular].
@tamayo2007metagene used a non-negative matrix factorization
approach to learn a low-dimensional approximation of the
microarray expression datasets and used the reduced space for comparisons.
Matrix factorization-based methods are commonly used for gene expression
analysis [@alter2000singular; @tamayo2007metagene].
An orthology independent matrix factorization framework based on generalized
singular value decomposition [GSVD; @van1976generalizing] was used
by @alter2003generalized to compare gene-expression
profiles from two species.
This framework was later extended to develop higher-order generalized singular
value decomposition (HO GSVD) to analyze data from more than two species 
[@ponnapalli2011higher].

This study developed a joint diagonalization approach called
approximate shared basis factorization (A-SBF)
for cross-species expression comparisons.
This approach extends the exact factorization approach we
developed called shared basis factorization (SBF). We discuss
the details of the two methods in the following sections.


# Shared basis factorization

Consider a set of real matrices $D_i \in \mathbb{R}^{m_i \times k}$
($i={1,\ldots ,N}$) with full column rank. We define 
shared basis factorization (SBF) as

\begin{align*}
  D_1 &= U_1\Delta_1V^T, \\
  D_2 &= U_2\Delta_2V^T, \\
      & \vdots \\
  D_N &= U_N\Delta_NV^T.
\end{align*}

Here each $U_i \in \mathbb{R}^{m_i \times k}$ is a dataset-specific
left basis matrix, each $\Delta_i \in \mathbb{R}^{k \times k}$ is a
diagonal matrix with positive values $\delta_{ik}$, and $V$ is
a square invertible matrix.

## Estimating the shared right basis matrix

Let $M$ be the scaled sum of the $D_i^T D_i$.
We define $M$ is defined as
\[
 M = \frac{\sum_{i=1}^{N} D_i^T D_i/w_i}{\alpha}.
\]

The scaling factor $w_i$ is the total variance explained by the column
vectors of $D_i$, and $\alpha$ is the inverse sum of the total variance of
$D_i$, for $i = 1 \cdots N$. The weights $w_i$ and $\alpha$ are defined as

\begin{align*}
    w_i &= \sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)} \mbox{ and}\\
    \alpha &= \sum_{i=1}^{N} \frac{1}{\sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)}}.
\end{align*}

Here $\sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)} = \tr(D_i^T D_i) = \tr(A_i)$.
Using the $w_i$ and $\alpha$, individual $D_i^T D_i$ are standardized.
If all the variances are equal, $M$ becomes the arithmetic mean of the sum of
$D_i^T D_i$.
The shared right basis matrix  $V$ is then determined from the eigenvalue
decomposition of $M$, where $M=V \Theta V^T$.
The shared right basis matrix $V$ is an orthogonal matrix as $M$ is symmetric.
Given $V$, we compute $U_i$ and $\Delta_i$ by solving the linear system
$D_i V = U_i \Delta_i = L_i$.
By normalizing the columns of $L_i$, we have
$\delta_{ik} = \|l_{ik}\|$ and
$\Delta_i = \mbox{diag}(\delta_{i1},\ldots,\delta_{ik})$.

# Approximate shared basis factorization

Consider a set of matrices $D_i \in \mathbb{R}^{m_i \times k}$
($i= 1,\ldots,N$), each with full column rank. We define
approximate shared basis factorization (A-SBF) as

\begin{align*}
  D_1 &= U_1\Delta_1V^T + \epsilon_1, \\
  D_2 &= U_2\Delta_2V^T + \epsilon_2, \\
      & \vdots \\
  D_N &= U_N\Delta_NV^T + \epsilon_N.
\end{align*}

Each $U_i \in \mathbb{R}^{m_i \times k}$ is a species-specific left basis
matrix with **orthonormal** columns (eigengenes),
$\Delta_i \in \mathbb{R}^{k \times k}$ is a diagonal matrix with positive
values $\Delta_{ik}$ and $V$ is a non singular square matrix.
The right basis matrix $V$ is identical in all the $N$ matrix factorizations
and defines the common space shared by all species. 
We learn the factorization such that the learned $V$ is closest to that in
the exact decomposition and by minimizing the total decomposition error 
$\sum^{N}_{i=1}\epsilon_i = {\sum^{N}_{i=1}\|D_i - U_i\Delta_iV^T\|^2}_F$.

# Usage cases

## SBF examples

```{r setup}
# load SBF package
library(SBF)
```

Let us create some random matrices using the `createRandomMatrices` function
from the SBF package. We will create four matrices, each with three columns with
rows varying from 4 to 6.

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
sapply(mymat, dim)
```
Rank of each of this matrices
```{r}
sapply(mymat, function(x) {
  qr(x)$rank
  })
```

Let us compute SBF using different approaches.

- Estimate $V$ using sum of $D_i^T D_i / N$
- Estimate $V$ using sum of $D_i^T D_i / N$ with inverse variance weighting
- Estimate $V$ using inter-sample correlation
```{r}
sbf <- SBF(matrix_list = mymat)
sbf_inv <- SBF(matrix_list = mymat, weighted = TRUE)
sbf_cor <- SBF(matrix_list = mymat, transform_matrix = TRUE)
```

When $D_i$'s are transformed to compute inter-sample correlation, we do not need
to scale it using inverse-variance weighting anymore. We recommend using inverse
variance weights, giving a more robust estimate of $V$ when noisy datasets are
present. We estimate $V$ using inter-sample correlation when dealing
with gene expression data sets.


`?SBF` help function shows all arguments for the SBF function.
Let us inspect the output of the `SBF` call.

```{r}
names(sbf)
```
`sbf$u`, `sbf$v`, and `sbf$delta` correspond to the estimated left basis
matrix, shared right basis matrix, and diagonal matrices.


The estimated $V \in R^{k \times k}$ has a dimension of $k \times k$,
where $k$ is the number of columns in $D_i$.

```{r}
sbf$v
```

The delta values for each matrix for the three cases are shown below.

```{r}
printDelta <- function(l) {
  for (eachmat in names(l$delta)) {
  cat(eachmat, ":", l$delta[[eachmat]], "\n")
  }
}
cat("sbf\n");printDelta(sbf)
cat("sbf_inv\n");printDelta(sbf_inv)
cat("sbf_cor\n");printDelta(sbf_cor)
```

The $V \in R^{k \times k}$ estimated in SBF is orthogonal.
So $V^T V = V V^T = I$.

```{r}
zapsmall(t(sbf$v) %*% sbf$v)
```

The estimated $V$ is an invertible matrix.

```{r}
qr(sbf$v)$rank
```

The $U_i$ matrices estimated in the SBF do not have orthonormal columns.
Let us explore that. 

```{r}
sapply(sbf$u, dim)
```
Let us take the first matrix $U_i \in R^{m_i \times k}$ to check this.
For this matrix, $U_i^T U_i$ will be $k \times k$ matrix where $k = 3$.

```{r}
t(sbf$u[[names(sbf$u)[1]]]) %*% sbf$u[[names(sbf$u)[1]]]
```


The estimated $M$ matrix is stored `sbf$m` and `sbf$lambda` gives the
eigenvalues in the eigenvalue decomposition ($M=V \Theta V^T$).

```{r}
sbf$lambda
```

SBF is an exact factorization. Let compute the factorization error for the 
three cases using `calcDecompError` function.
```{r}
calcDecompError(mymat, sbf$u, sbf$delta, sbf$v)
calcDecompError(mymat, sbf_inv$u, sbf_inv$delta, sbf_inv$v)
calcDecompError(mymat, sbf_cor$u, sbf_cor$delta, sbf_cor$v)
```
The errors are close to zero in all three cases.

### Adding new dataset

The total column variance of matrix 1-4 in `mymat` is approximately in the
same range.

```{r}
sapply(mymat, function(x) sum(diag(cov(x))))
```

Now, let us create two new matrix lists containing the `mymat`.
We will add a dataset with a similar variance to the first list and
a high variance to the second.

```{r}
mat5 <- matrix(c(130, 183, 62, 97, 147, 94, 102, 192, 19), byrow = T,
                    nrow = 3, ncol = 3)
mat5_highvar <- matrix(c(406, 319, 388, 292, 473, 287, 390, 533, 452),
                       byrow = T, nrow = 3, ncol = 3)

mymat_new <- mymat
mymat_new[["mat5"]] <- mat5
sapply(mymat_new, function(x) sum(diag(cov(x))))
mymat_new_noisy <- mymat
mymat_new_noisy[["mat5"]] <- mat5_highvar
sapply(mymat_new_noisy, function(x) sum(diag(cov(x))))
```

Let us compute SBF with the new datasets.

```{r}
sbf_new <- SBF(matrix_list = mymat_new)
sbf_inv_new <- SBF(matrix_list = mymat_new, weighted = TRUE)


sbf_new_noisy <- SBF(matrix_list = mymat_new_noisy)
sbf_inv_new_noisy <- SBF(matrix_list = mymat_new_noisy, weighted = TRUE)
```

Let us take the newly estimated values $U_i$, $\Delta_i$, and $V$ for the four
initial matrices in `mymat`. 
We will then compare the decomposition error for the two cases with and without
inverse variance weighting.

```{r}
e1 <- calcDecompError(mymat, sbf_new$u[1:4], sbf_new$delta[1:4], sbf_new$v)
e2 <- calcDecompError(mymat, sbf_new_noisy$u[1:4], sbf_new_noisy$delta[1:4],
                      sbf_new_noisy$v)
e2 / e1
```

```{r}
e3 <- calcDecompError(mymat, sbf_inv_new$u[1:4], sbf_inv_new$delta[1:4],
                      sbf_inv_new$v)
e4 <- calcDecompError(mymat, sbf_inv_new_noisy$u[1:4],
                      sbf_inv_new_noisy$delta[1:4], sbf_inv_new_noisy$v)
e4 / e3
```

With inverse variance weighting, the deviation is smaller.


## A-SBF examples

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
sapply(mymat, dim)
```

Now let us compute Approximate-SBF for the same datasets.

- A-SBF
- A-SBF with inverse variance weighting
- A-SBF with inter-sample correlation

```{r}
asbf <- SBF(matrix_list = mymat, approximate = TRUE)
asbf_inv <- SBF(matrix_list = mymat, weighted = TRUE, approximate = TRUE)
asbf_cor <- SBF(matrix_list = mymat, approximate = TRUE, transform_matrix = TRUE)
```


```{r}
names(asbf)
```

A-SBF is not an exact factorization. A-SBF output has two additional values.
`asbf$u_ortho` is the left basis matrix with orthonormal columns and
`asbf$error` gives the decomposition error.

```{r}
asbf$error
asbf_inv$error
asbf_cor$error
```

The same error can also be computed using the `calcDecompError` function.

```{r}
calcDecompError(mymat, asbf$u_ortho, asbf$delta, asbf$v)
calcDecompError(mymat, asbf_inv$u_ortho, asbf_inv$delta, asbf_inv$v)
calcDecompError(mymat, asbf_cor$u_ortho, asbf_cor$delta, asbf_cor$v)
```

In A-SBF factorization, $U_i$ has orthonormal columns, and $V$ is orthogonal.

```{r}
zapsmall(t(asbf$u_ortho[[names(asbf$u_ortho)[1]]]) %*%
           asbf$u_ortho[[names(asbf$u_ortho)[1]]])
```

```{r}
zapsmall(t(asbf$v) %*% asbf$v)
```

# Reduce A-SBF factorization error

We will use the following propositions to develop an iterative approach to
minimize the factorization error.

## Proposition I

Let $A \in \mathbb{R}^{m \times n}$ with rank $n$ and SVD of
$A Q^T = X \Sigma Y^T$.
Among all matrices $B \in \mathbb{R}^{m \times n}$ with orthonormal columns,
the Forbenius norm $\|A - B Q\|^2_F$ is minimized when $B = X Y^T$.


$$
\begin{aligned}
  \min \|A - B Q\|^2_F &= \tr\left( A - BQ \right)^T (A - BQ) \\
  &= \tr(A^T A) + \tr(Q^T Q) - 2 \tr(A Q^T B^T).  
\end{aligned}
$$

This is equivalent to maximizing $\tr(A Q^T B^T)$.
Let SVD of  $AQ^T = X \Sigma Y^T$, where $X \in R^{m \times m}$
is left singular matrix, 
$\Sigma \in R^{m \times n} = \begin{bmatrix}
           \Sigma_n \\
            0
            \end{bmatrix} \mbox{ with }
 \Sigma_n = \mbox{diag}(\sigma_1, \ldots, \sigma_n)$, and 
 $V \in R^{m \times n}$ is the right singular matrix.
Let $Z = Y^T B^T X$. Now $Z$ is a rectangular matrix with 
orthonormal rows ($Z Z^T = I$) as $B^T B = I$.
Now we have,

$$
\begin{aligned}
  \tr(A Q^T B^T) &= \tr(X \Sigma Y^T B^T)  =  \tr(Y^T B^T X \Sigma)
  = \tr(Z\Sigma) \\
  &= \sum_{i=1}^{n} z_{ii}\sigma_i \leq \sum_{i=1}^{n} \sigma_i.
  \end{aligned}
$$
The upper bound is obtained when $Z=I$ and thus $B=X Y^T$.

## Proposition II

Let $A \in \mathbb{R}^{m \times n}$ with rank $n$ and SVD of
$A^T B = X \Sigma Y^T$. Among all orthogonal matrices $Q$, the Forbenius norm
$\|A - B Q^T\|^2_F$ is minimized when $Q = X Y^T$.

$$
\begin{aligned}
  \min \|A - B Q^T \|^2_F &= \tr\left( A - BQ^T \right)^T (A - B Q^T) \\
  &= \tr(A^T A) + \tr(B^T B) - 2 \tr(A^T B Q^T).  
\end{aligned}
$$
This is equivalent to maximizing $\tr(A^T B Q^T)$. Let SVD of 
$A^T B = X \Sigma Y^T$, where
$\Sigma = \mbox{diag}(\sigma_1, \ldots, \sigma_n)$.
We define an orthogonal matrix $Z = Y^T Q X $. Now we have,

$$
\begin{aligned}
  \tr(A^T B Q^T) &= \tr(X \Sigma Y^T Q^T)  =  \tr(Y^T Q^T X \Sigma)
  = \tr(Z\Sigma)\\
  &= \sum_{i=1}^{n} z_{ii}\sigma_i \leq \sum_{i=1}^{p} \sigma_i.
\end{aligned}
$$
The upper bound is obtained when $Z=I$ and thus $Q = X Y^T$.

## Proposition III

For a set of $n$ matrices $A_1,A_2, \ldots, A_n$, where
$A_i \in \mathbb{R}^{k \times k}$, the  orthogonal matrix
$Q \in \mathbb{R}^{k \times k}$ minimizing the Forbenius norm 
$\sum_{i=1}^n \|A_i- Q \|^2_F$ is given by
$Q = X Y^T$, where SVD of $\sum_{i=1}^n A_i = X \Sigma Y^T$.
  
Let us first consider $i = 1$ case. We want to minimize the objective function:
$$
 \mathcal{F}(Q) = \|A_1- Q \|^2_F \mbox{ subject to } Q^T Q = I.
$$

The objective function can be reformulated as

$$
 \mathcal{F}(Q) =  \|A_1 - Q\|^2_{F} =  \tr\left(A_1^T A_1 - 2 A_1^T Q + Q^T Q \right).
$$

Let $\Phi$ be a matrix with Lagrange multipliers for constraint $Q^T Q - I = 0$.
The Lagrange $\mathcal{L}$ is

$$
 \mathcal{L}(Q) = \tr\left(A_1^T A_1 - 2 A_1^T Q + Q^T Q \right) +
 \tr \left( \Phi(Q^T Q - I) \right).
$$

The partial derivative of $\mathcal{L}$ with respect to $Q$ gives
$$
\begin{aligned}
    \frac{\partial \mathcal{L}}{\partial Q} &= -2 A_1 + Q ( \Phi^T + \Phi) \\
    A_1 &= Q \left( \frac{\Phi^T + \Phi}{2} \right) \\
    Q &= A_1 \left(\frac{\Phi^T + \Phi}{2} \right)^{-1}.
\end{aligned}
$$

Given that the $Q$ is orthogonal, we have

$$
\begin{aligned}
    A_1^T A_1 &= \left(\frac{\Phi^T + \Phi}{2} \right)^2 \\
    \left(\frac{\Phi^T + \Phi}{2} \right) &= (A_1^T A_1)^{1/2}.
\end{aligned}
$$

Thus for $i = 1$, $Q = A_1 (A_1^T A_1)^{-1/2}$. Now for $i = 2$, we have

$$
\begin{aligned}
    \frac{\partial \mathcal{L}}{\partial Q} &= -2 A_1 - 2 A_2 + Q ( \Phi^T + \Phi) \\
    A_1 + A_2 &= Q \left(\frac{\Phi^T + \Phi}{2} \right) \\
    Q &= (A_1 + A_2) \left(\frac{\Phi^T + \Phi}{2} \right)^{-1} \\
      &= (A_1 + A_2) \left( (A_1 + A_2)^T (A_1 + A_2) \right)^{-1/2}.
\end{aligned}
$$

Now for $i = n$, we have

$$
\begin{aligned}
  Q &= M ( M^T M)^{-1/2} \mbox{ where } M = \sum_{i=1}^n A_i = X \Sigma Y^T \\
    &=  X \Sigma Y^T ( Y \Sigma^2 Y^T)^{-1/2} \\
    &= X Y^T.
\end{aligned}
$$

## Iterative update

Using these three propositions, we iteratively update $U_i$, $\Delta_i$, and
$V$. The steps of the iterative algorithm are shown below.

1. Input: $D_i, i={1 \ldots N}$
2. Output: $U_i,\Delta_i$, and $V$, where $U_i^T U_i = I$ and $V^T V = V V^T = I$
3. Initialize $U_i^{k}$, $\Delta_i^{k}$, and $V^{k}$.
Compute $\epsilon^{k,k,k} = {\sum^{N}_{i=1}\|D_i - U_i^{k} \Delta_i^{k} {V^{k}}^{\!T}\|^{2}}_F$
4. Update $\mathbf{U_i^{k+1}}$:\
&nbsp;&nbsp;&nbsp;$U_i^{k+1} = Z Y^T$, where SVD of
$D_i V^{k} \Delta_i^{k} = Z \Sigma Y^T$.
Compute $\epsilon^{k+1,k,k}$ \
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k,k} < \epsilon^{k,k,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$U_i \leftarrow U_i^{k+1}$.
5. Update $\mathbf{\Delta_i^{k+1}}$: \
&nbsp;&nbsp;&nbsp;$\mathbf{\Delta_i^{k+1}} = \mbox{diag}( (U_i^{k+1})^T D_i V^k)$.
Compute $\epsilon^{k+1,k+1,k}$ \
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k+1,k} < \epsilon^{k+1,k,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta_i \leftarrow \Delta_i^{k+1}$
6. Update $\mathbf{V^{k+1}}$: \
&nbsp;&nbsp;&nbsp;$V^{k+1} =  M Q^T$, where SVD of
$\sum_{i}^N D_i^T U_i^{k+1} \Delta_i^{k+1} = M \Phi Q^T$. Compute
$\epsilon^{k+1,k+1,k+1}$\
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k+1,k+1} < \epsilon^{k+1,k+1,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V \leftarrow V^{k+1}$
7. Repeat steps 4-6 until convergence.

## Examples

### Optimizing A-SBF error

Let us optimize the factorization error using the `optimizeFactorization`
function for the three cases of A-SBF computation.

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
asbf <- SBF(matrix_list = mymat, approximate = TRUE)
asbf_inv <- SBF(matrix_list = mymat, weighted = TRUE, approximate = TRUE)
asbf_cor <- SBF(matrix_list = mymat, approximate = TRUE, transform_matrix = TRUE)
```


Depending upon the data matrix and initial values of $U_i$, $\Delta_i$, and $V$,
optimization could take some time.

```{r}
myopt <- optimizeFactorization(mymat, asbf$u_ortho, asbf$delta, asbf$v)
myopt_inv <- optimizeFactorization(mymat, asbf_inv$u_ortho, asbf_inv$delta,
                                   asbf_inv$v)
myopt_cor <- optimizeFactorization(mymat, asbf_cor$u_ortho, asbf_cor$delta,
                                   asbf_cor$v)
```


The number of iteration taken for optimizing and new factorization error:

```{r}
cat("For asbf, # iteration =", myopt$error_pos, "final error =", myopt$error)
cat("\nFor asbf inv, # iteration =", myopt_inv$error_pos, "final error =",
    myopt_inv$error)
cat("\nFor asbf cor, # iteration =", myopt_cor$error_pos, "final error =",
    myopt_cor$error)
```

```{r, echo = FALSE}
if ((round(myopt$error,2) == round(myopt_inv$error,2)) && (round(myopt$error,2) == round(myopt_cor$error,2))) {
  cat("After optimization, for all three A-SBF factorizations, the final error")
  cat("\nis the same (up to 2 decimals).")
  cat("\nThe final error is", round(myopt$error,2))  
}
```

### Using different initial values

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
```

Let us initialize the `optimizeFactorization` function with a random
orthogonal matrix and check the final optimization error. The $V$ matrix
estimated from the `mymat` matrix has a dimension of $3 \times 3$.
First we will create a random $3 \times 3$ matrix and obtain
an orthogonal matrix based on this.

```{r}
set.seed(111)
rand_mat <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
cat("\nRank is:", qr(rand_mat[[1]])$rank,"\n")
dim(rand_mat[[1]])
```
Get an orthogonal $V$ matrix using SVD. We will set $V$ as the right basis 
matrix from the SVD.

```{r}
mysvd <- svd(rand_mat[[1]])
randV <- mysvd$v
```

Now for this $V$, we will first compute $U_i$'s and $\Delta_i$ for different
$D_i$ matrices in the `mymat`. We achieve this by
solving the linear equations: $D_i = U_i \Delta_i V^T$ for $i = 1, \ldots, 4$.
We then orthonormalize the columns of $U_i$ using Proposition I.

```{r}
# get Ui and Delta for this newV
out <- computeUDelta(mymat, randV)
names(out)
```

The initial decomposition error is :

```{r}
calcDecompError(mymat, out$u_ortho, out$d, randV)
```

Now we will try to optimize using the new random $V$ and corresponding $U_i$'s
and $\Delta_i$'s.

```{r}
newopt <- optimizeFactorization(mymat, out$u_ortho, out$d, randV)
# Number of updates taken
newopt$error_pos
# New error
newopt$error
```

We achieve the same factorization error (```r newopt$error```) after the
`optimizeFactorization` function call.


Now instead of the right basis matrix from the SVD, we will
set $V$ as the left basis matrix.

```{r}
mysvd <- svd(rand_mat[[1]])
randV <- mysvd$u
dim(randV)
```

```{r}
# get Ui and Delta for this newV
out <- computeUDelta(mymat, randV)
calcDecompError(mymat, out$u_ortho, out$d, randV)
```

Now we will try to optimize with these matrices as our initial values.

```{r}
newopt <- optimizeFactorization(mymat, out$u_ortho, out$d, randV)
# Number of updates taken
newopt$error_pos
# New error
newopt$error
```

Again we get the same decomposition error after optimizing.

Instead of initial value being an orthogonal matrix, we will initialize with 
a random matrix that does not guarantee any orthogonal property.

```{r}
set.seed(111)
newv <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)[[1]]
k <- 2392
newu <- newd <- list()
for(i in names(mymat)){
  myrow <- nrow(mymat[[i]])
  mycol <- ncol(mymat[[i]])
  set.seed(k)
  newu[[i]] <- createRandomMatrices(n = 1, ncols = mycol, nrows = myrow)[[1]]
  set.seed(k*2)
  newd[[i]] <- sample(1:1000, size = mycol)
  newmat <- newu[[i]] %*% diag(newd[[i]]) %*% t(newv)
  if (!qr(newmat)$rank == mycol)
    cat("\nDi not full column rank")
  k = k+1
}
error <- calcDecompError(mymat, newu, newd, newv)
cat("\nInitial error = ", error,"\n")
newopt <- optimizeFactorization(mymat, newu, newd, newv)
newopt$error_pos
newopt$error
```
Again, we get the same factorization error after optimizing.

This shows that the iterative update procedure converges and achieves the
same decomposition error regardless of the initial values.

### Estimating SVD

We will further demonstrate the case for $N=1$ when we have just one matrix.
The `optimizeFactorization` function gives $U_i$'s with orthonormal column,
$\Delta_i$ a diagonal matrix, and an orthogonal $V$. If the function converges,
the results should be identical to a standard SVD, except for the sign 
changes corresponding to $U$ and $V$ columns.
So we will compare the results from the `optimizeFactorization`
function with the standard SVD output.
Let us generate one example matrix say `newmat`.

```{r}
set.seed(171)
newmat <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
newmat
```

We will estimate the SVD of `newmat` using our iterative update function from
another random matrix with the same dimension.

```{r}
set.seed(253)
randmat_new <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
randmat_new
newsvd <- svd(randmat_new[[1]])
```

Let us create a list for the $u$ and $\delta$ matrices we just obtained from the
SVD of the random matrix. This allows us to use these matrices as the initial
values for the `optimizeFactorization` function.

```{r}
newu <- newd <- list()
newu[[names(randmat_new)]] <- newsvd$u
newd[[names(randmat_new)]] <- newsvd$d
```

The factorization error
```{r}
calcDecompError(newmat, newu, newd, newsvd$v)
```

Let us optimize.

```{r}
opt_new <- optimizeFactorization(newmat, newu, newd, newsvd$v)
cat("\n # of updates:", opt_new$error_pos,"\n")
opt_new$error
```

Error is close to zero. Let us compare the original matrix with the
reconstructed matrix based on the estimated $u$, $d$ and $v$ using
the `optimizeFactorization` function.

```{r}
newmat
opt_new$u[[1]] %*% diag(opt_new$d[[1]]) %*% t(opt_new$v)
```
The estimated value is very close.

We can further improve our estimate by decreasing the tolerance parameter
(`tol`) in the optimization function.

```{r}
opt_new1 <- optimizeFactorization(newmat, newu, newd, newsvd$v, tol = 1e-21)
cat("\n # of updates:", opt_new1$error_pos,"\n")
opt_new1$error
opt_new1$u[[1]] %*% diag(opt_new1$d[[1]]) %*% t(opt_new1$v)
```
The reconstructed matrix is the same as the original matrix. Let us compare
the $U$ and $V$ with that from the standard SVD.

```{r}
newmat_svd <- svd(newmat[[1]])
```

```{r}
newmat_svd$u
opt_new1$u[[1]]
```
```{r}
newmat_svd$v
opt_new1$v
```
The results agree!

# Cross-species gene expression analysis using A-SBF

For cross-species gene expression datasets, we learn the common space $V$ based
on correlation ($R_i$) between column phenotypes
(such as tissues, cell types, etc.) within a species.
In our study, we have shown that the inter-tissue gene expression correlation
is similar across species.
Let $X_i \in \mathbb{R}^{m_i \times k}$ be a standardized gene expression
matrix where $X_i = C_i D_i {S_i}^{-1}$.
Here $C_i = I_{m_i} - {m_i}^{-1} 1_{m_i} {1_{m_i}}^T$ is a centering matrix and
$S_i = \mbox{diag}(s_1,\ldots,s_k)$ is a diagonal scaling matrix,
where $s_p$ is the standard deviation of $p$-th column of $D_i$.
The matrix $X_i$ is a matrix with columns of $D_i$ mean-centered and scaled
by the standard deviation.
The correlation between expression profiles of $k$ tissue types in species $i$
is given by $R_i = X_i^T X_i/m_i$.
We then define an expected correlation matrix ($\mathbb{E}(R_i)$) across $N$
species as $M$, where $M$ is defined as

\[
  M = \frac{\sum_{i=1}^{N} R_i}{N}.
\]

The shared right basis matrix $V$ capturing the inter-tissue gene expression
correlation is determined from the eigenvalue
decomposition of $M$, where $M=V \Theta V^T$.
Once the $V$ and $\Delta_i$ are learned using the SBF factorization,
we compute $U_i$ with orthonormal columns using proposition I.
The estimated $V$ space captures inter-tissue gene expression correlation
relationship.
For gene expression analysis, if we want the shared space to represent
inter-sample correlation relationship, we do not update/change $V$
while optimizing the factorization error.
In such cases, while reducing the factorization error
we set `optimizeV = FALSE` in the `optimizeFactorization` function.



## Usage examples

Let us load the SBF package's in-built gene expression dataset.
The dataset contains the average gene expression profile of five similar tissues
in three species.

```{r}
# load dataset
avg_counts <- SBF::TissueExprSpecies
# check the names of species
names(avg_counts)
```

```{r}
# head for first species
avg_counts[[names(avg_counts)[1]]][1:3, 1:3]
```

The number of genes annotated in different species is different. As a result,
the number of rows (genes) in the expression data will be different for
different species.

```{r}
sapply(avg_counts, dim)
```

Let us compute A-SBF with inter-tissue correlation.

```{r}
# A-SBF call using correlation matrix
asbf_cor <- SBF(matrix_list = avg_counts, check_col_matching = TRUE,
                col_index = 2, approximate = TRUE, transform_matrix = TRUE)
# decomposition error
asbf_cor$error
```

Optimize factorization to reduce decomposition error but by not updating $V$.
```{r}
myopt_gef <- optimizeFactorization(avg_counts, asbf_cor$u_ortho, asbf_cor$delta,
                                   asbf_cor$v, optimizeV = FALSE)
names(myopt_gef)
```

```{r}
# new error
myopt_gef$error
# number of iterations
myopt_gef$error_pos
```

The number of iterations taken to optimize = `r myopt_gef$error_pos`.

```{r}
identical(asbf_cor$v, myopt_gef$v)
```

Check whether estimated $U_i$'s have orthonormal columns.

```{r}
zapsmall(t(as.matrix(myopt_gef$u[[names(myopt_gef$u)[1]]])) %*%
           as.matrix(myopt_gef$u[[names(myopt_gef$u)[1]]]))
```

# References
