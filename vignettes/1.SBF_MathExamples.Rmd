---
title: "SBF vignette"
author: "Amal Thomas"
output:
  pdf_document:
    toc: yes
    number_section: yes
  knitr:::html_vignette:
    toc: yes
bibliography: references.bib
vignette: >
  %\VignetteIndexEntry{SBF_vignette}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

<style>
body {
text-align: justify}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


\newcommand{\diag}{\mathrm{diag}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}

# Background

Joint matrix factorization facilitates the comparison of
expression profiles from different species without using gene mapping.
Transforming gene expression profiles into reduced eigengene space using
singular value decomposition (SVD) has been shown to capture meaningful
biological information [@alter2000singular].
@tamayo2007metagene used a non-negative matrix factorization
approach to learn a low-dimensional approximation of the
microarray expression datasets and used the reduced space for comparisons.
Matrix factorization-based methods are commonly used for gene expression
analysis [@alter2000singular; @tamayo2007metagene].
An orthology independent matrix factorization framework based on generalized
singular value decomposition [GSVD; @van1976generalizing] was used
by @alter2003generalized to compare gene-expression
profiles from two species.
This framework was later extended to develop higher-order generalized singular
value decomposition (HO GSVD) to analyze data from more than two species 
[@ponnapalli2011higher].

This study developed a joint diagonalization approach called
approximate shared basis factorization (A-SBF)
for cross-species expression comparisons.
This approach extends the exact factorization approach we
developed called shared basis factorization (SBF). We discuss
the details of the two methods in the following sections.


# Shared basis factorization

Consider a set of real matrices $D_i \in \mathbb{R}^{m_i \times k}$
($i={1,\ldots ,N}$) with full column rank. We define 
shared basis factorization (SBF) as

\begin{align*}
  D_1 &= U_1\Delta_1V^T, \\
  D_2 &= U_2\Delta_2V^T, \\
      & \vdots \\
  D_N &= U_N\Delta_NV^T.
\end{align*}

Here each $U_i \in \mathbb{R}^{m_i \times k}$ is a dataset-specific
left basis matrix, each $\Delta_i \in \mathbb{R}^{k \times k}$ is a
diagonal matrix with positive values $\delta_{ik}$, and $V$ is
a square invertible matrix.

## Estimating the shared right basis matrix

Let $M$ be the scaled sum of the $D_i^T D_i$.
We define $M$ is defined as
\[
 M = \frac{\sum_{i=1}^{N} D_i^T D_i/w_i}{\alpha}.
\]

The scaling factor $w_i$ is the total variance explained by the column
vectors of $D_i$, and $\alpha$ is the inverse sum of the total variance of
$D_i$, for $i = 1 \cdots N$. The weights $w_i$ and $\alpha$ are defined as

\begin{align*}
    w_i &= \sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)} \mbox{ and}\\
    \alpha &= \sum_{i=1}^{N} \frac{1}{\sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)}}.
\end{align*}

Here $\sum_{j=1}^{k} \sigma_{jj}^{2\mbox{ }(i)} = \tr(D_i^T D_i) = \tr(A_i)$.
Using the $w_i$ and $\alpha$, individual $D_i^T D_i$ are standardized.
If all the variances are equal, $M$ becomes the arithmetic mean of the sum of
$D_i^T D_i$.
The shared right basis matrix  $V$ is then determined from the eigenvalue
decomposition of $M$, where $M=V \Theta V^T$.
The shared right basis matrix $V$ is an orthogonal matrix as $M$ is symmetric.
Given $V$, we compute $U_i$ and $\Delta_i$ by solving the linear system
$D_i V = U_i \Delta_i = L_i$.
By normalizing the columns of $L_i$, we have
$\delta_{ik} = \|l_{ik}\|$ and
$\Delta_i = \mbox{diag}(\delta_{i1},\ldots,\delta_{ik})$.

# Approximate shared basis factorization

Consider a set of matrices $D_i \in \mathbb{R}^{m_i \times k}$
($i= 1,\ldots,N$), each with full column rank. We define
approximate shared basis factorization (A-SBF) as

\begin{align*}
  D_1 &= U_1\Delta_1V^T + \epsilon_1, \\
  D_2 &= U_2\Delta_2V^T + \epsilon_2, \\
      & \vdots \\
  D_N &= U_N\Delta_NV^T + \epsilon_N.
\end{align*}

Each $U_i \in \mathbb{R}^{m_i \times k}$ is a species-specific left basis
matrix with **orthonormal** columns (eigengenes),
$\Delta_i \in \mathbb{R}^{k \times k}$ is a diagonal matrix with positive
values $\Delta_{ik}$ and $V$ is a non singular square matrix.
The right basis matrix $V$ is identical in all the $N$ matrix factorizations
and defines the common space shared by all species. 
We estimate the factorization such that the estimated $V$ is closest to that in
the exact decomposition and by minimizing the total decomposition error 
$\sum^{N}_{i=1}\epsilon_i = {\sum^{N}_{i=1}\|D_i - U_i\Delta_iV^T\|^2}_F$.

# Usage cases

## SBF examples

```{r setup}
# load SBF package
library(SBF)
```

Let us create some random matrices using the `createRandomMatrices` function
from the SBF package. We will create four matrices, each with three columns with
rows varying from 4 to 6.

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
sapply(mymat, dim)
```
Rank of each of this matrices
```{r}
sapply(mymat, function(x) {
  qr(x)$rank
  })
```

Let us compute SBF using different approaches.

- Estimate $V$ using sum of $D_i^T D_i / N$
- Estimate $V$ using sum of $D_i^T D_i / N$ with inverse variance weighting
- Estimate $V$ using inter-sample correlation
```{r}
sbf <- SBF(matrix_list = mymat)
sbf_inv <- SBF(matrix_list = mymat, weighted = TRUE)
sbf_cor <- SBF(matrix_list = mymat, transform_matrix = TRUE)
```

When $D_i$'s are transformed to compute inter-sample correlation, we do not need
to scale it using inverse-variance weighting anymore. We recommend using inverse
variance weights, giving a more robust estimate of $V$ when noisy datasets are
present. We estimate $V$ using inter-sample correlation when dealing
with gene expression data sets.


`?SBF` help function shows all arguments for the SBF function.
Let us inspect the output of the `SBF` call.

```{r}
names(sbf)
```
`sbf$u`, `sbf$v`, and `sbf$delta` correspond to the estimated left basis
matrix, shared right basis matrix, and diagonal matrices.


The estimated $V \in R^{k \times k}$ has a dimension of $k \times k$,
where $k$ is the number of columns in $D_i$.

```{r}
sbf$v
```

The delta values for each matrix for the three cases are shown below.

```{r}
printDelta <- function(l) {
  for (eachmat in names(l$delta)) {
  cat(eachmat, ":", l$delta[[eachmat]], "\n")
  }
}
cat("sbf\n");printDelta(sbf)
cat("sbf_inv\n");printDelta(sbf_inv)
cat("sbf_cor\n");printDelta(sbf_cor)
```

The $V \in R^{k \times k}$ estimated in SBF is orthogonal.
So $V^T V = V V^T = I$.

```{r}
zapsmall(t(sbf$v) %*% sbf$v)
```

The estimated $V$ is an invertible matrix.

```{r}
qr(sbf$v)$rank
```

The $U_i$ matrices estimated in the SBF do not have orthonormal columns.
Let us explore that. 

```{r}
sapply(sbf$u, dim)
```
Let us take the first matrix $U_i \in R^{m_i \times k}$ to check this.
For this matrix, $U_i^T U_i$ will be $k \times k$ matrix where $k = 3$.

```{r}
t(sbf$u[[names(sbf$u)[1]]]) %*% sbf$u[[names(sbf$u)[1]]]
```


The estimated $M$ matrix is stored `sbf$m` and `sbf$lambda` gives the
eigenvalues in the eigenvalue decomposition ($M=V \Theta V^T$).

```{r}
sbf$lambda
```

SBF is an exact factorization. Let compute the factorization error for the 
three cases using `calcDecompError` function.
```{r}
calcDecompError(mymat, sbf$u, sbf$delta, sbf$v)
calcDecompError(mymat, sbf_inv$u, sbf_inv$delta, sbf_inv$v)
calcDecompError(mymat, sbf_cor$u, sbf_cor$delta, sbf_cor$v)
```
The errors are close to zero in all three cases.

### Adding new dataset

The total column variance of matrix 1-4 in `mymat` is approximately in the
same range.

```{r}
sapply(mymat, function(x) sum(diag(cov(x))))
```

Now, let us create two new matrix lists containing the `mymat`.
We will add a dataset with a similar variance to the first list and
a high variance to the second.

```{r}
mat5 <- matrix(c(130, 183, 62, 97, 147, 94, 102, 192, 19), byrow = T,
                    nrow = 3, ncol = 3)
mat5_highvar <- matrix(c(406, 319, 388, 292, 473, 287, 390, 533, 452),
                       byrow = T, nrow = 3, ncol = 3)

mymat_new <- mymat
mymat_new[["mat5"]] <- mat5
sapply(mymat_new, function(x) sum(diag(cov(x))))
mymat_new_noisy <- mymat
mymat_new_noisy[["mat5"]] <- mat5_highvar
sapply(mymat_new_noisy, function(x) sum(diag(cov(x))))
```

Let us compute SBF with the new datasets.

```{r}
sbf_new <- SBF(matrix_list = mymat_new)
sbf_inv_new <- SBF(matrix_list = mymat_new, weighted = TRUE)


sbf_new_noisy <- SBF(matrix_list = mymat_new_noisy)
sbf_inv_new_noisy <- SBF(matrix_list = mymat_new_noisy, weighted = TRUE)
```

Let us take the newly estimated values $U_i$, $\Delta_i$, and $V$ for the four
initial matrices in `mymat`. 
We will then compare the decomposition error for the two cases with and without
inverse variance weighting.

```{r}
e1 <- calcDecompError(mymat, sbf_new$u[1:4], sbf_new$delta[1:4], sbf_new$v)
e2 <- calcDecompError(mymat, sbf_new_noisy$u[1:4], sbf_new_noisy$delta[1:4],
                      sbf_new_noisy$v)
e2 / e1
```

```{r}
e3 <- calcDecompError(mymat, sbf_inv_new$u[1:4], sbf_inv_new$delta[1:4],
                      sbf_inv_new$v)
e4 <- calcDecompError(mymat, sbf_inv_new_noisy$u[1:4],
                      sbf_inv_new_noisy$delta[1:4], sbf_inv_new_noisy$v)
e4 / e3
```

With inverse variance weighting, the deviation is smaller.


## A-SBF examples

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
sapply(mymat, dim)
```

Now let us compute Approximate-SBF for the same datasets.

- A-SBF
- A-SBF with inverse variance weighting
- A-SBF with inter-sample correlation

```{r}
asbf <- SBF(matrix_list = mymat, approximate = TRUE)
asbf_inv <- SBF(matrix_list = mymat, weighted = TRUE, approximate = TRUE)
asbf_cor <- SBF(matrix_list = mymat, approximate = TRUE, transform_matrix = TRUE)
```


```{r}
names(asbf)
```

A-SBF is not an exact factorization. A-SBF output has two additional values.
`asbf$u_ortho` is the left basis matrix with orthonormal columns and
`asbf$error` gives the decomposition error.

```{r}
asbf$error
asbf_inv$error
asbf_cor$error
```

The same error can also be computed using the `calcDecompError` function.

```{r}
calcDecompError(mymat, asbf$u_ortho, asbf$delta, asbf$v)
calcDecompError(mymat, asbf_inv$u_ortho, asbf_inv$delta, asbf_inv$v)
calcDecompError(mymat, asbf_cor$u_ortho, asbf_cor$delta, asbf_cor$v)
```

In A-SBF factorization, $U_i$ has orthonormal columns, and $V$ is orthogonal.

```{r}
zapsmall(t(asbf$u_ortho[[names(asbf$u_ortho)[1]]]) %*%
           asbf$u_ortho[[names(asbf$u_ortho)[1]]])
```

```{r}
zapsmall(t(asbf$v) %*% asbf$v)
```

# Minimizing A-SBF factorization error


Here we provide details on minimizing the decomposition error of
the A-SBF method.
In A-SBF, we want to solve the following optimization problem:
$$
\begin{array}{rl}
  \mbox{minimize:} & \sum^{n}_{i=1} \|D_i - U_i\Delta_iV^T\|^2_F,\\[0.5em]
  \mbox{subject to:} & U_i^TU_i = I, \mbox{ for } 1\leq i \leq n,\\
  & V^TV = VV^T = I,\\
  & \Delta_i = \diag(\delta_{i,1},\ldots,\delta_{i,k}),\\
  & \delta_{i,j} \geq 0, \mbox{ for } 1\leq i\leq n.
  \end{array}
$$

The objective function can be reformulated as
$$
\textstyle
\sum_{i=1}^n
\tr(D_i^TD_i)
- 2\tr\big(D_i^TU_i\Delta_iV^T\big)
+ \tr(\Delta_i^2)
$$
Let $\Phi$, $\Psi_i$, and $\Theta_i$ be matrices with Lagrange
multipliers for constraints $V^TV - I = 0$, $U_i^TU_i - I = 0$ and
$\delta_{i,j} \geq 0$. The Lagrange $\mathcal{L}$ is

$$
\mathcal{L}(U, \Delta, V) = \sum_{i=1}^n \tr(D_i^T D_i - 2 D_i^T U_i \Delta_i V^T + \Delta_i^2) +
\tr (\Phi(V^T V - I)) + \tr (\Psi_i(U_i^T U_i - I)) + \tr(\Theta_i \Delta_i).
$$

Solving for an individual $U_i$ using the partial derivative of
$\mathcal{L}$ with respect to $U_i$ gives

$$
\begin{aligned}
  \frac{\partial}{\partial U_i} \mathcal{L}(U, \Delta, V) &= -2D_iV\Delta_i + U_i(\Psi_i^T + \Psi_i)\\
  U_i &= D_iV\Delta_i((D_iV\Delta_i)^T(D_iV\Delta_i))^{-1/2}. \label{eqU}
\end{aligned}
$$


Solving $V$ using the partial derivative of $\mathcal{L}$ with respect
to $V$ gives

$$
\begin{aligned}
  \frac{\partial}{\partial V} \mathcal{L}(U, \Delta, V)
  &= \sum_{i=1}^n -2D_i^TU_i\Delta_i + V(\Phi^T + \Phi) \\
  V &= \sum_{i=1}^n D_i^TU_i\Delta_i ( (\sum_{i=1}^n D_i^TU_i\Delta_i)^T (\sum_{i=1}^n D_i^TU_i\Delta_i))^{-1/2}. \label{eqV}
\end{aligned}
$$

Solving for an individual $\Delta_i$ using the partial derivative of $\mathcal{L}$ with
respect to $\Delta_i$ gives

$$
\begin{aligned}
\frac{\partial}{\partial \Delta_i} \mathcal{L}(U, \Delta, V) &= -2 U_i^T D_iV + 2\Delta_i  + \Theta_i \\
  \Delta_i &= U_i^T D_i V. \label{eqDelta}
\end{aligned}
$$

In practice, we do not compute the square root inverse to update $U_i$ and $V$.
Now we show that we make the optimal update of our matrix
in each iteration based on the singular value decomposition (SVD)
of other matrices.
The first two results below show that, given one matrix factor,
we can obtain the other as required in our algorithm.

## Proposition I

  Consider matrices $A \in \mathbb{R}^{m \times n}$ with rank $n$ and
  $Q\in \mathbb{R}^{n\times n}$. If $X\Sigma Y^T$ is the SVD of
  $AQ^T$, then among matrices $B\in\mathbb{R}^{m\times n}$ with
  orthonormal columns, $\|A - BQ\|_F$ is minimized when $B=XY^T$.



  The norm and its square have the same minimum value, so we consider
  the minimum of
$$
\|A - B Q\|^2_F = \tr(A^T A) + \tr(Q^T Q) - 2 \tr(A Q^T B^T),
$$

  which coincides with the maximum value of $\tr(A Q^T B^T)$. With $X
  \Sigma Y^T$ the SVD of $AQ^T$, define $Z = Y^TB^TX$. Since $B^TB=I$,
  we have $Z Z^T = I$. We can bound the maximum value of $\tr(A Q^T B^T)$
  as follows:

$$
\tr(AQ^TB^T) = \tr(X\Sigma Y^TB^T)
    = \tr(Z\Sigma) =
    \sum_{i=1}^{n} Z_{ii}\sigma_i \leq \sum_{i=1}^{n} \sigma_i,
$$
and this bound is attained when $Z=I$, and thus $B=XY^T$.

In proposition \ref{orthoU} we substitute $A = D_i, B = U_i,$
and $Q = \Delta_i V^T$. Now we have
$AQ^T = D_i V \Delta_i  = X \Sigma Y^T$. The $U_i$ with orthonormal columns
is given by $XY^T$. The same can also be obtained by substituting 
$D_i V \Delta_i  = X \Sigma Y^T$ in equation 1.

$$
\begin{aligned}
    U_i &= D_iV\Delta_i((D_iV\Delta_i)^T(D_iV\Delta_i))^{-1/2} \\
    U_i &= X \Sigma Y^T((X \Sigma Y^T)^T(X \Sigma Y^T))^{-1/2} = X Y^T.
\end{aligned}
$$

## Proposition II

  Consider matrices $A \in \mathbb{R}^{m \times n}$ with rank $n$ and
  $B\in \mathbb{R}^{m\times n}$. If $X\Sigma Y^T$ is the SVD of
  $A^TB$, then among orthogonal matrices $Q\in\mathbb{R}^{n\times n}$,
  $\|A - BQ^T\|_F$ is minimized when $Q=XY^T$.


  The norm and its square have the same minimum value, so we consider
  the minimum of
$$
\|A - BQ^T\|^2_F = \tr(A^TA) + \tr(B^TB) - 2\tr(A^TBQ^T),
$$
which coincides with the maximum value of $\tr(A^TBQ^T)$. With $X\Sigma Y^T$
  the SVD of $A^TB$, define orthogonal matrix $Z = Y^TQ^TX$. We bound the
  maximum of $\tr(A^TBQ^T)$ as

$$
\tr(A^TBQ^T) = \tr(X\Sigma Y^TQ^T)
    = \tr(Z\Sigma)
    = \sum_{i=1}^{n} Z_{ii}\sigma_i \leq \sum_{i=1}^{n} \sigma_i,
$$
and this bound is attained when $Z=I$ and thus $Q=XY^T$.


In proposition II, we substitute
$A = D_i, B = U_i \Delta_i,$ and $Q = V_i$. Now we have
$A^TB = D_i^T U_i \Delta_i  = X \Sigma Y^T$ and orthogonal $V_i$
is given by $XY^T$. In A-SBF, $V$ is shared across the $n$ factorizations.
To find the shared $V$, we can use the following proposition.


## Proposition III

  For a set of $n$ matrices $A_1,A_2, \ldots, A_n$, where $A_i \in
  \mathbb{R}^{k \times k}$, the orthogonal matrix $Q \in
  \mathbb{R}^{k\times k}$ minimizing $\sum_{i=1}^n\|A_i- Q\|^2_F$ is
  given by $Q = X Y^T$, where SVD of $\sum_{i=1}^nA_i=X\Sigma Y^T$.
  
We seek to minimize
$$
 \sum_{i=1}^n\|A_i - Q\|^2_F ~~\mbox{subject to}~~ Q^TQ=I.
$$

  Defining $A = \sum_{i=1}^n A_i$, this objective function can be
  rewritten

$$
\tr(\sum_{i=1}^nA_i^TA_i) - 2\tr(A^TQ) + n\tr(Q^TQ).
$$

  Let $\Phi$ be a matrix of Lagrange multipliers for $Q^TQ-I=0$. The
  Lagrangian $\mathcal{L}$ is then
$$
  \mathcal{L}(Q,\Phi) = \tr(\sum_{i=1}^nA_i^TA_i) - 2\tr(A^TQ) + \tr(Q^TQ) + \tr(\Phi(Q^TQ-I)).
$$

Since $Q$ is orthogonal, the partial derivatives of
  $\mathcal{L}(Q,\Phi)$ with respect to $Q$ are as follows:
$$
\frac{\partial \mathcal{L}(Q,\Phi)}{\partial Q} = -2A + Q(\Phi^T + \Phi).
$$

  Setting this equation to $0$ yields:


$$
  Q = A\left(\frac{\Phi^T + \Phi}{2}\right)^{-1} ~~\mbox{and}~~ A = Q\left(\frac{\Phi^T + \Phi}{2}\right).
$$

Again because $Q$ is orthogonal,

$$
A^TA = \left(\frac{\Phi^T + \Phi}{2} \right)^2 ~~\mbox{and}~~
    \left(\frac{\Phi^T + \Phi}{2}\right) = (A^TA)^{1/2},
$$
  which implies $Q = A(A^TA)^{-1/2}$. Since $X\Sigma Y^T$ is the SVD
  of $A$ we conclude

$$
Q = X\Sigma Y^T(Y\Sigma^2 Y^T)^{-1/2} = XY^T.
$$
In proposition \ref{orthoVsum}, we substitute
$A_i = V_i = D_i^T U_i \Delta_i$ and $Q = V$.
Now we have $\sum_{i=1}^n A_i = \sum_{i=1}^n D_i^T U_i \Delta_i = X \Sigma Y^T$
and the shared basis in A-SBF is given by $V = X Y^T$.


## Iterative update

Using these three propositions, we iteratively update $U_i$, $\Delta_i$, and
$V$. The steps of the iterative algorithm are shown below.

1. Input: $D_i, i={1 \ldots N}$
2. Output: $U_i,\Delta_i$, and $V$, where $U_i^T U_i = I$ and $V^T V = V V^T = I$
3. Initialize $U_i^{k}$, $\Delta_i^{k}$, and $V^{k}$.
Compute $\epsilon^{k,k,k} = {\sum^{N}_{i=1}\|D_i - U_i^{k} \Delta_i^{k} {V^{k}}^{\!T}\|^{2}}_F$
4. Update $\mathbf{U_i^{k+1}}$:\
&nbsp;&nbsp;&nbsp;$U_i^{k+1} = Z Y^T$, where SVD of
$D_i V^{k} \Delta_i^{k} = Z \Sigma Y^T$.
Compute $\epsilon^{k+1,k,k}$ \
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k,k} < \epsilon^{k,k,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$U_i \leftarrow U_i^{k+1}$.
5. Update $\mathbf{\Delta_i^{k+1}}$: \
&nbsp;&nbsp;&nbsp;$\mathbf{\Delta_i^{k+1}} = \mbox{diag}( (U_i^{k+1})^T D_i V^k)$.
Compute $\epsilon^{k+1,k+1,k}$ \
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k+1,k} < \epsilon^{k+1,k,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\Delta_i \leftarrow \Delta_i^{k+1}$
6. Update $\mathbf{V^{k+1}}$: \
&nbsp;&nbsp;&nbsp;$V^{k+1} =  M Q^T$, where SVD of
$\sum_{i}^N D_i^T U_i^{k+1} \Delta_i^{k+1} = M \Phi Q^T$. Compute
$\epsilon^{k+1,k+1,k+1}$\
&nbsp;&nbsp;&nbsp;If $\epsilon^{k+1,k+1,k+1} < \epsilon^{k+1,k+1,k}$: \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$V \leftarrow V^{k+1}$
7. Repeat steps 4-6 until convergence.

Our iterative approach is a block-coordinate descent algorithm.
In gradient descent, we have the general
update:
$$
\theta_{t+1} = \theta_t - \eta \nabla \mathcal{F}(\theta_t),
$$
where $t$ is the iteration counter, $\eta$ is the learning rate 
and $\nabla \mathcal{F} (\theta)$ is the gradient of the cost function.
In A-SBF, the gradient of the cost function with respect to $U$ is
$-2 D V \Delta$. So we have

$$
  U_{t+1} = U_t + \eta D V \Delta.
$$
By setting $\eta  = I - U_t (D V \Delta)^{-1}$, we have $U_{t+1} = D V \Delta$.
Since we require orthonormal columns, we find the closest orthogonal matrix to
$D V \Delta$, and set the final value of $U_{t+1} = Z Y^T$, where SVD of
$DV\Delta = Z \Sigma Y^T$.
This approach employs orthogonal Procrustes solution along with the
gradient descent algorithm.
We can also achieve this by directly setting
$\eta = \frac{Z Y^T - U_t}{D V \Delta}$, so that 
$U_{t+1}$ has all the properties we need.
The learning rate determines the size of the steps. If the value of $U_t$ is very different
from $Z Y^T$, $\eta$ will be high, and we take larger steps.
As it becomes closer, the learning rate also decreases.
We have a similar case for updating $V$ and $\Delta$.


## Examples

### Optimizing A-SBF error

Let us optimize the factorization error using the `optimizeFactorization`
function for the three cases of A-SBF computation.

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
asbf <- SBF(matrix_list = mymat, approximate = TRUE)
asbf_inv <- SBF(matrix_list = mymat, weighted = TRUE, approximate = TRUE)
asbf_cor <- SBF(matrix_list = mymat, approximate = TRUE, transform_matrix = TRUE)
```


Depending upon the data matrix and initial values of $U_i$, $\Delta_i$, and $V$,
optimization could take some time.

```{r}
myopt <- optimizeFactorization(mymat, asbf$u_ortho, asbf$delta, asbf$v)
myopt_inv <- optimizeFactorization(mymat, asbf_inv$u_ortho, asbf_inv$delta,
                                   asbf_inv$v)
myopt_cor <- optimizeFactorization(mymat, asbf_cor$u_ortho, asbf_cor$delta,
                                   asbf_cor$v)
```


The number of iteration taken for optimizing and new factorization error:

```{r}
cat("For asbf, # iteration =", myopt$error_pos, "final error =", myopt$error)
cat("\nFor asbf inv, # iteration =", myopt_inv$error_pos, "final error =",
    myopt_inv$error)
cat("\nFor asbf cor, # iteration =", myopt_cor$error_pos, "final error =",
    myopt_cor$error)
```

```{r, echo = FALSE}
if ((round(myopt$error,2) == round(myopt_inv$error,2)) && (round(myopt$error,2) == round(myopt_cor$error,2))) {
  cat("After optimization, for all three A-SBF factorizations, the final error")
  cat("\nis the same (up to 2 decimals).")
  cat("\nThe final error is", round(myopt$error,2))  
}
```

### Using different initial values

```{r}
set.seed(1231)
mymat <- createRandomMatrices(n = 4, ncols = 3, nrows = 4:6)
```

1. Let us initialize the `optimizeFactorization` function with a random
orthogonal matrix and check the final optimization error. The $V$ matrix
estimated from the `mymat` matrix has a dimension of $3 \times 3$.
First we will create a random $3 \times 3$ matrix and obtain
an orthogonal matrix based on this.

```{r}
set.seed(111)
rand_mat <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
cat("\nRank is:", qr(rand_mat[[1]])$rank,"\n")
dim(rand_mat[[1]])
```
Get an orthogonal $V$ matrix using SVD. We will set $V$ as the right basis 
matrix from the SVD.

```{r}
mysvd <- svd(rand_mat[[1]])
randV <- mysvd$v
```

Now for this $V$, we will first compute $U_i$'s and $\Delta_i$ for different
$D_i$ matrices in the `mymat`. We achieve this by
solving the linear equations: $D_i = U_i \Delta_i V^T$ for $i = 1, \ldots, 4$.
We then orthonormalize the columns of $U_i$ using Proposition I.

```{r}
# get Ui and Delta for this newV
out <- computeUDelta(mymat, randV)
names(out)
```

The initial decomposition error is :

```{r}
calcDecompError(mymat, out$u_ortho, out$d, randV)
```

Now we will try to optimize using the new random $V$ and corresponding $U_i$'s
and $\Delta_i$'s.

```{r}
newopt <- optimizeFactorization(mymat, out$u_ortho, out$d, randV)
# Number of updates taken
newopt$error_pos
# New error
newopt$error
```

We achieve the same factorization error (```r newopt$error```) after the
`optimizeFactorization` function call.


2. Now instead of the right basis matrix from the SVD, we will
set $V$ as the left basis matrix.

```{r}
mysvd <- svd(rand_mat[[1]])
randV <- mysvd$u
dim(randV)
```

```{r}
# get Ui and Delta for this newV
out <- computeUDelta(mymat, randV)
calcDecompError(mymat, out$u_ortho, out$d, randV)
```

Now we will try to optimize with these matrices as our initial values.

```{r}
newopt <- optimizeFactorization(mymat, out$u_ortho, out$d, randV)
# Number of updates taken
newopt$error_pos
# New error
newopt$error
```

Again we get the same decomposition error after optimizing.

3. Instead of initial value being an orthogonal matrix, we will initialize
$U_i$'s, $\Delta_i$, and $V$ with random matrices such that it 
does not guarantee
- orthogonal property for $V$ and
- orthonormal columns for $U_i$'s.

```{r}
set.seed(111)
# new random v
newv <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)[[1]]
# seed value
k <- 2392
newu <- newd <- list()
for(i in names(mymat)){
  myrow <- nrow(mymat[[i]])
  mycol <- ncol(mymat[[i]])
  set.seed(k)
  # new random u_i
  newu[[i]] <- createRandomMatrices(n = 1, ncols = mycol, nrows = myrow)[[1]]
  set.seed(k*2)
  # new random d_i
  newd[[i]] <- sample(1:1000, size = mycol)
  newmat <- newu[[i]] %*% diag(newd[[i]]) %*% t(newv)
  if (!qr(newmat)$rank == mycol)
    cat("\nNew matrix does not have full column rank")
  k = k+1
}
error <- calcDecompError(mymat, newu, newd, newv)
cat("\nInitial error = ", error,"\n")
```

We see a very high factorization error because of the random initialization.

```{r}
newopt <- optimizeFactorization(mymat, newu, newd, newv)
newopt$error_pos
newopt$error
```

Again, we get the same factorization error after optimizing. Try changing
the seed value and compare the results.

This shows that the iterative update procedure converges and achieves the
same decomposition error regardless of the initial values.

### Estimating SVD

We will further demonstrate the case for $N=1$ when we have just one matrix.
The `optimizeFactorization` function gives $U_i$'s with orthonormal column,
$\Delta_i$ a diagonal matrix, and an orthogonal $V$. If the function converges,
the results should be identical to a standard SVD, except for the sign 
changes corresponding to $U$ and $V$ columns.
So we will compare the results from the `optimizeFactorization`
function with the standard SVD output.
Let us generate one example matrix say `newmat`.

```{r}
set.seed(171)
newmat <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
newmat
```

1. We will estimate the SVD of `newmat` using our iterative update function by
setting the initial values to be identity matrix

```{r}
newu <- newd <- list()
newu[["mat1"]]  <- diag(3)
newd[["mat1"]] <- diag(newu[["mat1"]])
newu
newd
```
The factorization error when initializing using identity matrix:

```{r}
calcDecompError(newmat, newu, newd, diag(3))
```

Let us optimize.

```{r}
opt_new <- optimizeFactorization(newmat, newu, newd, diag(3))
cat("\n # of updates:", opt_new$error_pos,"\n")
opt_new$error
```
Error is close to zero. Let us compare the original matrix with the
reconstructed matrix based on the estimated $u$, $d$ and $v$ using
the `optimizeFactorization` function.

```{r}
newmat
opt_new$u[[1]] %*% diag(opt_new$d[[1]]) %*% t(opt_new$v)
```
```{r}
opt_new1 <- optimizeFactorization(newmat, newu, newd, diag(3), tol = 1e-21)
cat("\n # of updates:", opt_new1$error_pos,"\n")
opt_new1$error
```
```{r}
newmat
opt_new1$u[[1]] %*% diag(opt_new1$d[[1]]) %*% t(opt_new1$v)
```
The reconstructed matrix is the same as the original matrix. Let us compare
the $U$ and $V$ with that from the standard SVD.


```{r}
newmat_svd <- svd(newmat[[1]])
```

```{r}
newmat_svd$d
opt_new1$d
```


```{r}
newmat_svd$u
opt_new1$u[[1]]
```
```{r}
newmat_svd$v
opt_new1$v
```
The results agree except for the sign and ordering of columns.

2. Now we will estimate the SVD of `newmat` using our iterative update function from
another random matrix with the same dimension.

```{r}
set.seed(253)
randmat_new <- createRandomMatrices(n = 1, ncols = 3, nrows = 3)
randmat_new
newsvd <- svd(randmat_new[[1]])
```

Let us create a list for the $u$ and $\delta$ matrices we just obtained from the
SVD of the random matrix. This allows us to use these matrices as the initial
values for the `optimizeFactorization` function.

```{r}
newu <- newd <- list()
newu[[names(randmat_new)]] <- newsvd$u
newd[[names(randmat_new)]] <- newsvd$d
```

The factorization error
```{r}
calcDecompError(newmat, newu, newd, newsvd$v)
```

Let us optimize.

```{r}
opt_new <- optimizeFactorization(newmat, newu, newd, newsvd$v)
cat("\n # of updates:", opt_new$error_pos,"\n")
opt_new$error
```

Error is close to zero. Let us compare the original matrix with the
reconstructed matrix based on the estimated $u$, $d$ and $v$ using
the `optimizeFactorization` function.

```{r}
newmat
opt_new$u[[1]] %*% diag(opt_new$d[[1]]) %*% t(opt_new$v)
```
The estimated value is very close.

We can further improve our estimate by decreasing the tolerance parameter
(`tol`) in the optimization function.

```{r}
opt_new1 <- optimizeFactorization(newmat, newu, newd, newsvd$v, tol = 1e-21)
cat("\n # of updates:", opt_new1$error_pos,"\n")
opt_new1$error
opt_new1$u[[1]] %*% diag(opt_new1$d[[1]]) %*% t(opt_new1$v)
```
The reconstructed matrix is the same as the original matrix. Let us compare
the $U$ and $V$ with that from the standard SVD.

```{r}
newmat_svd <- svd(newmat[[1]])
```

```{r}
newmat_svd$u
opt_new1$u[[1]]
```
```{r}
newmat_svd$v
opt_new1$v
```
The results agree!

# Cross-species gene expression analysis using A-SBF

For cross-species gene expression datasets, we estimate the common space $V$ based
on correlation ($R_i$) between column phenotypes
(such as tissues, cell types, etc.) within a species.
In our study, we have shown that the inter-tissue gene expression correlation
is similar across species.
Let $X_i \in \mathbb{R}^{m_i \times k}$ be a standardized gene expression
matrix where $X_i = C_i D_i {S_i}^{-1}$.
Here $C_i = I_{m_i} - {m_i}^{-1} 1_{m_i} {1_{m_i}}^T$ is a centering matrix and
$S_i = \mbox{diag}(s_1,\ldots,s_k)$ is a diagonal scaling matrix,
where $s_p$ is the standard deviation of $p$-th column of $D_i$.
The matrix $X_i$ is a matrix with columns of $D_i$ mean-centered and scaled
by the standard deviation.
The correlation between expression profiles of $k$ tissue types in species $i$
is given by $R_i = X_i^T X_i/m_i$.
We then define an expected correlation matrix ($\mathbb{E}(R_i)$) across $N$
species as $M$, where $M$ is defined as

\[
  M = \frac{\sum_{i=1}^{N} R_i}{N}.
\]

The shared right basis matrix $V$ capturing the inter-tissue gene expression
correlation is determined from the eigenvalue
decomposition of $M$, where $M=V \Theta V^T$.
Once the $V$ and $\Delta_i$ are estimated using the SBF factorization,
we compute $U_i$ with orthonormal columns using proposition I.
The estimated $V$ space captures inter-tissue gene expression correlation
relationship.
For gene expression analysis, if we want the shared space to represent
inter-sample correlation relationship, we do not update/change $V$
while optimizing the factorization error.
In such cases, while reducing the factorization error
we set `optimizeV = FALSE` in the `optimizeFactorization` function.



## Usage examples

Let us load the SBF package's in-built gene expression dataset.
The dataset contains the average gene expression profile of five similar tissues
in three species.

```{r}
# load dataset
avg_counts <- SBF::TissueExprSpecies
# check the names of species
names(avg_counts)
```

```{r}
# head for first species
avg_counts[[names(avg_counts)[1]]][1:3, 1:3]
```

The number of genes annotated in different species is different. As a result,
the number of rows (genes) in the expression data will be different for
different species.

```{r}
sapply(avg_counts, dim)
```

Let us compute A-SBF with inter-tissue correlation.

```{r}
# A-SBF call using correlation matrix
asbf_cor <- SBF(matrix_list = avg_counts, check_col_matching = TRUE,
                col_index = 2, approximate = TRUE, transform_matrix = TRUE)
# decomposition error
asbf_cor$error
```

Optimize factorization to reduce decomposition error but by not updating $V$.
```{r}
myopt_gef <- optimizeFactorization(avg_counts, asbf_cor$u_ortho, asbf_cor$delta,
                                   asbf_cor$v, optimizeV = FALSE)
names(myopt_gef)
```

```{r}
# new error
myopt_gef$error
# number of iterations
myopt_gef$error_pos
```

The number of iterations taken to optimize = `r myopt_gef$error_pos`.

```{r}
identical(asbf_cor$v, myopt_gef$v)
```

Check whether estimated $U_i$'s have orthonormal columns.

```{r}
zapsmall(t(as.matrix(myopt_gef$u[[names(myopt_gef$u)[1]]])) %*%
           as.matrix(myopt_gef$u[[names(myopt_gef$u)[1]]]))
```

# References
